{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "1:\n Feature selection is an important step in anomaly detection as it helps to reduce the dimensionality of \nthe data by selecting the most relevant features. This can improve the accuracy of the anomaly detection\nalgorithm by reducing noise and irrelevant data.    ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "2:\n Common evaluation metrics for anomaly detection algorithms include precision, recall, F1 score, and area under the receiver \noperating characteristic curve (AUC-ROC). Precision measures the proportion of true positives among the total number of positive\npredictions, while recall measures the proportion of true positives among the total number of actual positives. F1 score is the\nharmonic mean of precision and recall. AUC-ROC is a measure of the trade-off between true positive rate and false positive rate.\nThese metrics can be computed using a confusion matrix or through cross-validation techniques   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "3:\n  DBSCAN is a density-based unsupervised machine learning algorithm used for clustering multi-dimensional data based on model parameters \nlike epsilon and minimum samples. The algorithm groups data points into clusters and is robust to outliers. It works by first selecting\na random point and finding all the neighboring points within a specified distance (epsilon). If the number of neighboring points is greater\nthan or equal to a specified minimum (minPoints), the algorithm creates a new cluster and adds all the neighboring points to the cluster.\nThe algorithm then repeats this process for all the points in the cluster until all points have been assigned to a cluster. Points that are\nnot assigned to any cluster are considered outliers.  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "4:\n  The epsilon parameter in DBSCAN specifies the maximum distance between two points for them to be considered as part of the same cluster. \nWhen it comes to detecting anomalies, a higher value of epsilon can lead to more points being grouped together, potentially masking the outliers.\nOn the other hand, a lower value of epsilon can lead to too many small clusters, making it difficult to identify the anomalies. Therefore, it is\nimportant to choose an appropriate epsilon value based on the data and the specific anomaly detection task at hand to achieve optimal performance.  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "5:\n In DBSCAN, core points are those that have at least a minimum number of points (MinPts) within\ntheir radius. Border points are those that are within the radius of a core point, but do not have \nenough neighboring points to be considered core themselves. Noise points are those that are neither core nor border points.\n\nIn the context of anomaly detection, core points are considered as normal points since they belong to high-density regions.\nBorder points are borderline cases, and noise points are considered as anomalies since they do not belong to any high-density regions.\nTherefore, anomalies can be identified as noise points in DBSCAN.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "6:\n DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that can also be used to detect anomalies in a dataset.\nIt detects anomalies by identifying data points that are not part of any cluster, i.e., they have low density and are far from other data points. The key\nparameters involved in the process are epsilon (eps), which defines the radius within which a minimum number of points (min_samples) must be present for a\ncluster to form, and min_samples, which specifies the minimum number of points required to form a dense region.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "7:\n The make_circles package in scikit-learn is a function that generates a dataset consisting of concentric circles. This dataset is commonly \nused for testing and visualizing clustering algorithms, as it contains two classes that cannot be separated linearly.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "8:\n Local outliers are data points that have a lower density than their surrounding neighbors, while global outliers are data points\nthat have a lower density than the entire dataset. Local outliers are often caused by noise or measurement errors, while global \noutliers are caused by fundamental differences in the data distribution. Local outliers can be detected using density-based clustering \nalgorithms like DBSCAN, while global outliers require more sophisticated techniques like Isolation Forest or One-Class SVM.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "9:\n   Local outliers can be detected using the Local Outlier Factor (LOF) algorithm by measuring the deviation of data points\nin a local neighborhood. The algorithm computes a score for each data point based on the density of its surrounding neighbors,\nwhere a low score indicates an outlier. Points with a score greater than 1 are considered normal, while points with a score less than 1 are considered outliers.  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "10:\n Global outliers can be detected using the Isolation Forest algorithm by recursively partitioning data points into\nsubsets using random feature splits. The algorithm computes an anomaly score for each point based on the average depth\nof the trees in which it appears. Points with a low score are considered outliers. The intuition behind the algorithm \nis that outliers are more likely to be isolated in smaller partitions than normal points.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "11:\n Local outlier detection is more appropriate than global outlier detection in scenarios where the data has a high degree of heterogeneity, \nand outliers are expected to be present in specific regions of the feature space. For example, detecting anomalies in sensor networks or \nidentifying fraudulent transactions in credit card data. In contrast, global outlier detection is more appropriate when the data is more \nhomogeneous, and the outliers are expected to be uniformly distributed across the feature space. For instance, identifying rare diseases in\na large medical dataset.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}